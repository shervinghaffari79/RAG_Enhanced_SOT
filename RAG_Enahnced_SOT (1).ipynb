{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hWTf8azu3OwO",
    "outputId": "a831e51f-5a9d-4220-b1ba-a29d3448125b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'SoT'...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/SimonAytes/SoT.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jtjNnpf87b3P",
    "outputId": "ea72a2c5-6ce1-4bc8-c6ba-5e69d66fb21f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/SoT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.0.1 (from -r requirements.txt (line 1))\n",
      "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n",
      "Collecting transformers==4.30.0 (from -r requirements.txt (line 2))\n",
      "  Downloading transformers-4.30.0-py3-none-any.whl.metadata (113 kB)\n",
      "Collecting tokenizers==0.13.3 (from -r requirements.txt (line 3))\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: numpy==1.26.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (1.26.4)\n",
      "Collecting loguru==0.7.3 (from -r requirements.txt (line 5))\n",
      "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (4.13.2)\n",
      "Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (1.14.0)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.0.0 (from torch==2.0.1->-r requirements.txt (line 1))\n",
      "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers==4.30.0->-r requirements.txt (line 2))\n",
      "  Downloading huggingface_hub-0.32.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.30.0->-r requirements.txt (line 2)) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.30.0->-r requirements.txt (line 2)) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.30.0->-r requirements.txt (line 2))\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.30.0->-r requirements.txt (line 2)) (2.32.3)\n",
      "Collecting safetensors>=0.3.1 (from transformers==4.30.0->-r requirements.txt (line 2))\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.30.0->-r requirements.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: setuptools in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r requirements.txt (line 1)) (78.1.1)\n",
      "Requirement already satisfied: wheel in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r requirements.txt (line 1)) (0.45.1)\n",
      "Collecting cmake (from triton==2.0.0->torch==2.0.1->-r requirements.txt (line 1))\n",
      "  Downloading cmake-4.0.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.3 kB)\n",
      "Collecting lit (from triton==2.0.0->torch==2.0.1->-r requirements.txt (line 1))\n",
      "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.0->-r requirements.txt (line 2)) (2025.5.1)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.0->-r requirements.txt (line 2))\n",
      "  Downloading hf_xet-1.1.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch==2.0.1->-r requirements.txt (line 1)) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers==4.30.0->-r requirements.txt (line 2)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers==4.30.0->-r requirements.txt (line 2)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers==4.30.0->-r requirements.txt (line 2)) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers==4.30.0->-r requirements.txt (line 2)) (2025.4.26)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->torch==2.0.1->-r requirements.txt (line 1)) (1.3.0)\n",
      "Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m204.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.30.0-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m275.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m212.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
      "Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m338.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m298.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m199.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m186.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m177.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m340.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m329.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m260.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m256.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m382.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m227.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.32.4-py3-none-any.whl (512 kB)\n",
      "Downloading hf_xet-1.1.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m341.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m191.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading cmake-4.0.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m241.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
      "Installing collected packages: tokenizers, lit, safetensors, regex, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, loguru, hf-xet, cmake, nvidia-cusolver-cu11, nvidia-cudnn-cu11, huggingface-hub, transformers, triton, torch\n",
      "\u001b[2K  Attempting uninstall: triton━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m19/22\u001b[0m [transformers]ub]1]11]1]\n",
      "\u001b[2K    Found existing installation: triton 3.3.01m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m19/22\u001b[0m [transformers]\n",
      "\u001b[2K    Uninstalling triton-3.3.0:━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m19/22\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled triton-3.3.0[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m19/22\u001b[0m [transformers]\n",
      "\u001b[2K  Attempting uninstall: torch━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m20/22\u001b[0m [triton]s]\n",
      "\u001b[2K    Found existing installation: torch 2.7.0+cu128\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m20/22\u001b[0m [triton]\n",
      "\u001b[2K    Uninstalling torch-2.7.0+cu128:━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m21/22\u001b[0m [torch]\n",
      "\u001b[2K      Successfully uninstalled torch-2.7.0+cu1280m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m21/22\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/22\u001b[0m [torch]m21/22\u001b[0m [torch]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "lightning 2.5.1.post0 requires torch<4.0,>=2.1.0, but you have torch 2.0.1 which is incompatible.\n",
      "pytorch-lightning 2.5.1.post0 requires torch>=2.1.0, but you have torch 2.0.1 which is incompatible.\n",
      "torchvision 0.22.0+cu128 requires torch==2.7.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed cmake-4.0.2 hf-xet-1.1.3 huggingface-hub-0.32.4 lit-18.1.8 loguru-0.7.3 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.13.3 torch-2.0.1 transformers-4.30.0 triton-2.0.0\n",
      "Obtaining file:///teamspace/studios/this_studio/SoT\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers==4.30.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sketch_of_thought==0.1.0) (4.30.0)\n",
      "Requirement already satisfied: torch==2.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sketch_of_thought==0.1.0) (2.0.1)\n",
      "Requirement already satisfied: tqdm>=4.67.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sketch_of_thought==0.1.0) (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.29.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sketch_of_thought==0.1.0) (0.32.4)\n",
      "Requirement already satisfied: tokenizers==0.13.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sketch_of_thought==0.1.0) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sketch_of_thought==0.1.0) (0.5.3)\n",
      "Requirement already satisfied: requests>=2.32.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sketch_of_thought==0.1.0) (2.32.3)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sketch_of_thought==0.1.0) (6.0.2)\n",
      "Requirement already satisfied: filelock>=3.17.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sketch_of_thought==0.1.0) (3.18.0)\n",
      "Requirement already satisfied: regex>=2024.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sketch_of_thought==0.1.0) (2024.11.6)\n",
      "Requirement already satisfied: numpy==1.26.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sketch_of_thought==0.1.0) (1.26.4)\n",
      "Requirement already satisfied: loguru==0.7.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sketch_of_thought==0.1.0) (0.7.3)\n",
      "Requirement already satisfied: typing-extensions in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.0.1->sketch_of_thought==0.1.0) (4.13.2)\n",
      "Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.0.1->sketch_of_thought==0.1.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.0.1->sketch_of_thought==0.1.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.0.1->sketch_of_thought==0.1.0) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.0.1->sketch_of_thought==0.1.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.0.1->sketch_of_thought==0.1.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.0.1->sketch_of_thought==0.1.0) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.0.1->sketch_of_thought==0.1.0) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.0.1->sketch_of_thought==0.1.0) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.0.1->sketch_of_thought==0.1.0) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.0.1->sketch_of_thought==0.1.0) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.0.1->sketch_of_thought==0.1.0) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.0.1->sketch_of_thought==0.1.0) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.0.1->sketch_of_thought==0.1.0) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.0.1->sketch_of_thought==0.1.0) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.0.1->sketch_of_thought==0.1.0) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->sketch_of_thought==0.1.0) (78.1.1)\n",
      "Requirement already satisfied: wheel in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->sketch_of_thought==0.1.0) (0.45.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.30.0->sketch_of_thought==0.1.0) (24.2)\n",
      "Requirement already satisfied: cmake in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from triton==2.0.0->torch==2.0.1->sketch_of_thought==0.1.0) (4.0.2)\n",
      "Requirement already satisfied: lit in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from triton==2.0.0->torch==2.0.1->sketch_of_thought==0.1.0) (18.1.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.29.0->sketch_of_thought==0.1.0) (2025.5.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.29.0->sketch_of_thought==0.1.0) (1.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.0->sketch_of_thought==0.1.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.0->sketch_of_thought==0.1.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.0->sketch_of_thought==0.1.0) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.0->sketch_of_thought==0.1.0) (2025.4.26)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch==2.0.1->sketch_of_thought==0.1.0) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->torch==2.0.1->sketch_of_thought==0.1.0) (1.3.0)\n",
      "Building wheels for collected packages: sketch_of_thought\n",
      "  Building editable for sketch_of_thought (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sketch_of_thought: filename=sketch_of_thought-0.1.0-0.editable-py3-none-any.whl size=7583 sha256=2e4f2d8debf311be8c5e3f0e5df822a2399d3ee5ff36dc309e50024a13d5c0ab\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ze4eb1f7/wheels/74/1a/68/8e109723e8fc09eb117e2092ed33edb9bf61932d0ee27bb272\n",
      "Successfully built sketch_of_thought\n",
      "Installing collected packages: sketch_of_thought\n",
      "Successfully installed sketch_of_thought-0.1.0\n",
      "Requirement already satisfied: transformers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.30.0)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.32.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2025.4.26)\n",
      "Downloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m351.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m332.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
      "\u001b[2K  Attempting uninstall: tokenizers\n",
      "\u001b[2K    Found existing installation: tokenizers 0.13.3\n",
      "\u001b[2K    Uninstalling tokenizers-0.13.3:\n",
      "\u001b[2K      Successfully uninstalled tokenizers-0.13.3\n",
      "\u001b[2K  Attempting uninstall: transformers\n",
      "\u001b[2K    Found existing installation: transformers 4.30.0\n",
      "\u001b[2K    Uninstalling transformers-4.30.0:\n",
      "\u001b[2K      Successfully uninstalled transformers-4.30.0━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [transformers][0m [transformers]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sketch-of-thought 0.1.0 requires tokenizers==0.13.3, but you have tokenizers 0.21.1 which is incompatible.\n",
      "sketch-of-thought 0.1.0 requires transformers==4.30.0, but you have transformers 4.52.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed tokenizers-0.21.1 transformers-4.52.4\n",
      "Collecting torch==2.4.0\n",
      "  Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting torchvision==0.19.0\n",
      "  Downloading torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
      "Collecting torchaudio==2.4.0\n",
      "  Downloading torchaudio-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.4.0) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.4.0) (4.13.2)\n",
      "Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.4.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.4.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.4.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch==2.4.0) (2025.5.1)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.0)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch==2.4.0)\n",
      "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torchvision==0.19.0) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torchvision==0.19.0) (11.2.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0) (12.8.61)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch==2.4.0) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->torch==2.4.0) (1.3.0)\n",
      "Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl (797.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m179.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m270.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchaudio-2.4.0-cp310-cp310-manylinux1_x86_64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m128.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m270.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m258.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m288.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m188.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m257.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m385.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m393.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m395.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m368.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m394.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m255.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, torchaudio\n",
      "\u001b[2K  Attempting uninstall: triton\n",
      "\u001b[2K    Found existing installation: triton 2.0.0\n",
      "\u001b[2K    Uninstalling triton-2.0.0:\n",
      "\u001b[2K      Successfully uninstalled triton-2.0.0\n",
      "\u001b[2K  Attempting uninstall: nvidia-nvtx-cu12━━━━━━━━\u001b[0m \u001b[32m 0/15\u001b[0m [triton]\n",
      "\u001b[2K    Found existing installation: nvidia-nvtx-cu12 12.8.5515\u001b[0m [triton]\n",
      "\u001b[2K    Uninstalling nvidia-nvtx-cu12-12.8.55:━━\u001b[0m \u001b[32m 0/15\u001b[0m [triton]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nvtx-cu12-12.8.550/15\u001b[0m [triton]\n",
      "\u001b[2K  Attempting uninstall: nvidia-nccl-cu12━━━━\u001b[0m \u001b[32m 0/15\u001b[0m [triton]\n",
      "\u001b[2K    Found existing installation: nvidia-nccl-cu12 2.26.2/15\u001b[0m [triton]\n",
      "\u001b[2K    Uninstalling nvidia-nccl-cu12-2.26.2:━━━\u001b[0m \u001b[32m 0/15\u001b[0m [triton]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nccl-cu12-2.26.2 0/15\u001b[0m [triton]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusparse-cu12━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/15\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cusparse-cu12 12.5.7.53[0m \u001b[32m 2/15\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cusparse-cu12-12.5.7.53:━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/15\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusparse-cu12-12.5.7.53━━━━━\u001b[0m \u001b[32m 3/15\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-curand-cu12━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/15\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-curand-cu12 10.3.9.55━\u001b[0m \u001b[32m 3/15\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-curand-cu12-10.3.9.55:━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/15\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-curand-cu12-10.3.9.55━━━\u001b[0m \u001b[32m 3/15\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cufft-cu12━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/15\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cufft-cu12 11.3.3.41━━\u001b[0m \u001b[32m 4/15\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cufft-cu12-11.3.3.41:━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/15\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cufft-cu12-11.3.3.41━━━━━━━━\u001b[0m \u001b[32m 5/15\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-runtime-cu12━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/15\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-runtime-cu12 12.8.57m \u001b[32m 5/15\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-runtime-cu12-12.8.57:━━━━━━━━━━━━\u001b[0m \u001b[32m 5/15\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-runtime-cu12-12.8.57[0m \u001b[32m 5/15\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-nvrtc-cu12━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/15\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-nvrtc-cu12 12.8.61[0m \u001b[32m 6/15\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-nvrtc-cu12-12.8.61:━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/15\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.8.61━━━━━\u001b[0m \u001b[32m 7/15\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-cupti-cu12━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/15\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-cupti-cu12 12.8.57[0m \u001b[32m 7/15\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-cupti-cu12-12.8.57:━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/15\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-cupti-cu12-12.8.57━\u001b[0m \u001b[32m 7/15\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cublas-cu12m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/15\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cublas-cu12 12.8.3.14━\u001b[0m \u001b[32m 8/15\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cublas-cu12-12.8.3.14:━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/15\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cublas-cu12-12.8.3.14━━━━━━━\u001b[0m \u001b[32m 9/15\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusolver-cu120m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/15\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cusolver-cu12 11.7.2.55[0m \u001b[32m 9/15\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cusolver-cu12-11.7.2.55:━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/15\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusolver-cu12-11.7.2.55━\u001b[0m \u001b[32m 9/15\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cudnn-cu121m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m10/15\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cudnn-cu12 9.7.1.26━━━\u001b[0m \u001b[32m10/15\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cudnn-cu12-9.7.1.26:\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m10/15\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cudnn-cu12-9.7.1.26━━━━━━━━━\u001b[0m \u001b[32m11/15\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K  Attempting uninstall: torch━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m11/15\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K    Found existing installation: torch 2.0.1[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m11/15\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K    Uninstalling torch-2.0.1:━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m12/15\u001b[0m [torch]nn-cu12]\n",
      "\u001b[2K      Successfully uninstalled torch-2.0.190m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m12/15\u001b[0m [torch]\n",
      "\u001b[2K  Attempting uninstall: torchvision━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m12/15\u001b[0m [torch]\n",
      "\u001b[2K    Found existing installation: torchvision 0.22.0+cu128━━━━━\u001b[0m \u001b[32m12/15\u001b[0m [torch]\n",
      "\u001b[2K    Uninstalling torchvision-0.22.0+cu128:\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m13/15\u001b[0m [torchvision]\n",
      "\u001b[2K      Successfully uninstalled torchvision-0.22.0+cu1280m━━━━━\u001b[0m \u001b[32m13/15\u001b[0m [torchvision]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/15\u001b[0m [torchaudio]5\u001b[0m [torchaudio]]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sketch-of-thought 0.1.0 requires tokenizers==0.13.3, but you have tokenizers 0.21.1 which is incompatible.\n",
      "sketch-of-thought 0.1.0 requires torch==2.0.1, but you have torch 2.4.0 which is incompatible.\n",
      "sketch-of-thought 0.1.0 requires transformers==4.30.0, but you have transformers 4.52.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.4.0 torchaudio-2.4.0 torchvision-0.19.0 triton-3.0.0\n"
     ]
    }
   ],
   "source": [
    "%cd SoT\n",
    "!pip install -r requirements.txt\n",
    "!pip install -e .\n",
    "!pip install --upgrade transformers\n",
    "!pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jjFNh-ybMgpF",
    "outputId": "6b9971f2-fe74-41e3-c234-860a27732943"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.40.2\n",
      "  Downloading transformers-4.40.2-py3-none-any.whl.metadata (137 kB)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.40.2) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.40.2) (0.32.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.40.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.40.2) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.40.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.40.2) (2024.11.6)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.40.2) (2.32.3)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers==4.40.2)\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.40.2) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers==4.40.2) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2) (1.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers==4.40.2) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers==4.40.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers==4.40.2) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers==4.40.2) (2025.4.26)\n",
      "Downloading transformers-4.40.2-py3-none-any.whl (9.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m211.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m332.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
      "\u001b[2K  Attempting uninstall: tokenizers\n",
      "\u001b[2K    Found existing installation: tokenizers 0.21.1\n",
      "\u001b[2K    Uninstalling tokenizers-0.21.1:\n",
      "\u001b[2K      Successfully uninstalled tokenizers-0.21.1\n",
      "\u001b[2K  Attempting uninstall: transformers\n",
      "\u001b[2K    Found existing installation: transformers 4.52.4\n",
      "\u001b[2K    Uninstalling transformers-4.52.4:╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled transformers-4.52.4━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [transformers][0m [transformers]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sketch-of-thought 0.1.0 requires tokenizers==0.13.3, but you have tokenizers 0.19.1 which is incompatible.\n",
      "sketch-of-thought 0.1.0 requires torch==2.0.1, but you have torch 2.4.0 which is incompatible.\n",
      "sketch-of-thought 0.1.0 requires transformers==4.30.0, but you have transformers 4.40.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed tokenizers-0.19.1 transformers-4.40.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers==4.40.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ORH60hwB3Qxa"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sketch_of_thought import SoT\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544,
     "referenced_widgets": [
      "0edc94621a93495a82b3f794f22dc088",
      "aa9da7977aa54c2fbc186f05b99b2b4b",
      "1f22d499d24d4f958d3e9b01c11fed26",
      "559f7830897a4717a80792c981a23140",
      "0e5d4fad0a864687a4a8c5431f3e25fe",
      "ac5a531e731e4728856d4aee4c72ead3",
      "863ca2cfd7d94d29ad206135f01b4b00",
      "40f585769c6b439790f3958ed8cb5c8e",
      "bfaac2bc1cb84cd4b0a281eece1dcbd0",
      "359a9e6aa0244cb582819eff8073409f",
      "67659047e0664189963c00777cb23e12",
      "287bf084dadb4fc28f4e12290550135d",
      "ed60e5c0420c43a29935b1f0f98d73cd",
      "c83e95320c3e4e7f981a6c3395e56413",
      "1e18fb576c5341a797f2976b93083567",
      "347798a6e8d4481994403d3181ad262c",
      "a0103dce9f63494c9151cece74ecd1b0",
      "9cc00eda7d654394879e47b7773ab2bc",
      "4aaaa012dc9c4153babd085fdd327a2d",
      "da95310ded27490cbc5fadec1a0fb021",
      "9f5c1db60551497897fe4c4d1d7521d6",
      "3bb6fb8571c2426480acfd4ab9f78946",
      "96bd8a5c73ad4d31bb2eddea01293de2",
      "b5acce17ee9d4c458d30e47300b34bfc",
      "a94e41876c5d47769f72466e6a4c3421",
      "dff20f3a699549d5b4402db30f6d0720",
      "ffc38f690b0b4357a93a1a0efdeb51a4",
      "39851731015e402ba99babe997542d8f",
      "61c673337b92430bb5a5db98dabb11b3",
      "043fc5ff138f47a185c9f1eff3449fbd",
      "22531ce7b67d4567acf1c967de042e27",
      "6fff62f34e784c76b77219ff8247eb9d",
      "a21617182b2b42f3bfe53c5a95a68f7f",
      "c959a9823488489bb2f22b47c951c9e3",
      "ab063249517846c2b14830e0c73bf2a3",
      "137f65ad3b414cd2856f658be634d375",
      "280d17af15684a5797a9f34f1c6c00d0",
      "72f4788446e541df8c206ad35ae9d829",
      "bae4d15ecd58450dac6dbc225a0883d2",
      "f72d0feb81454dcb8fa2d45d7862c3a4",
      "fc77af29a9bc4f199a60501250d1c08f",
      "752a926ef8b64cf58ba7ef768c13e0e9",
      "092488bad9874ad291c0581dfd6e8c75",
      "20f277ccf4bb4ea1b20b761b1b4ba6c6",
      "27972f5fc42c40978a16eaa4040cae6f",
      "52ce361d1d0f401aa6f55d01726afad2",
      "cd49d122f5404e1e9574774d1bb3af7b",
      "f5b4cffdaf4f42788c90294ec95d3984",
      "db360d49a6b245369229d963a729c87d",
      "d617764916c944549e1d12de7ca697b9",
      "9fc88edea9b54d98b0f7c63e39d5ccb7",
      "7affa41f6cc04c67be1bf2f7634824f6",
      "b6490f0a12af4271bcb6bf17e6aab59c",
      "f0db2efd24904f09aec4b2889f6b935d",
      "f2f35747ea7a4fb1a2196c54cfb3f8cd",
      "78ee1cb931ee459eb0692162b7e6ecde",
      "a735222f183d4603848dd889aa574bd6",
      "4789716411624322897cd3e953645032",
      "b6203b8865b34f11a9e428c18c0299a4",
      "0610812e4f764ad3b7c70e691042ff9e",
      "8a9a0690e9f74e8588109de567767628",
      "514548eb711844c7999764cb5c0292d2",
      "08d06d1cbe0e4dae9ec7704b87352b50",
      "564414325fa14761be8c51b3750df8c2",
      "dfec62ff39844c6d96c9b3278b15eda6",
      "9589dd0a2ac64279b85d0be744526814",
      "f22d5ad6a68449a4af0cd4e502457a3e",
      "2897ddc51fe34f6da197a987c370af7e",
      "2ac60e8b7a6e4ad1b269234053f60348",
      "a8cbd066ea6449af81f27d33514e7c0c",
      "8b4251136bc64162a771984eed1b55a8",
      "46c26e92b98c420aba7f14567adf178a",
      "05349c36cd2f4f9d9e9f14591adf1ff2",
      "7d5f66214dc94821ae4a30cb533de6db",
      "c792845626bd4cc581dbfb99942d3e6b",
      "d2cef355731e40548ae1a4ea3e1accb4",
      "9bd50b6ca8884244bac71e00f597742a",
      "c11412d165fd4380b9392eabf88af442",
      "3ba46107c5c64a30bbd13526c06aab8f",
      "f7039a7aa4cd4151ac3ba59ff60a4ae6",
      "3597e78f86cb49faa3b5dd8a394c214e",
      "9b801df0838f47c4b7ca7d7400aa5608",
      "28e6e807c4284b3c8e6f986574ab14a6",
      "f0404a50377549758eef6bdef5d23844",
      "2ec43733cfba4692ae31450e1fa39ec7",
      "e9763ce1b908421c949beb8efab138b2",
      "b6211beb59f94b0893076fcf453a1aa1",
      "93ad41a5a2584e93aff46e6afeb6d717"
     ]
    },
    "id": "arAu26yj3XEC",
    "outputId": "45249c1f-f344-4cb6-ab12-58dda8ec44e6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ceb9703743c4c11971ae1151811e7e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ce2015a1cb34c2292b1447c86fdc5c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/27.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aae78a5babcd445d8875c443d5ac9caf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d82bbb7de274036a4c60e839350412d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "203fb8f8011345a883c32ca14b7dab8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e36cdfb00cd142afa2994e352bae3c62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb2fbda84d9e4a748264cb3b89a5887b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8e1cf9b58d44a0dae69dc6afcae4ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "410ccb30d6714b7a8b9e13a5ac0f67eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed8c320cd49d4efa977b5ad558b51024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e00ac23745e4e9baf9c8dd2a2d64de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63616ae8b5294b468decaa3744b0bae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af360962b6824774b3ce1f2c09f2bb7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    "    torch_dtype=\"auto\",  # or torch.float16 if supported\n",
    "    device_map=None      # Avoid triggering TP logic\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Kc0YfQNqHPXa"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "2SaznWQRciwZ"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7607c8fb2eb049e681177252d9c8af08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/769 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e44c9f9cbff943d5bd8e43c16469a920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3285f031c0874f35b661df843455f689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.25k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "321fa52f39d44448b10d9c31c21def8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cdcdfce12064ea8a172b8473e222b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sot = SoT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3pfg0U0LCL2p"
   },
   "outputs": [],
   "source": [
    "prompt = \"Alice has 5 apples. She gives 3 apples to Bob. How many apples does Alice have?\"\n",
    "\n",
    "# Classify and get appropriate context\n",
    "paradigm = sot.classify_question(prompt)\n",
    "messages = sot.get_initialized_context(\n",
    "    paradigm,\n",
    "    prompt,\n",
    "    format=\"llm\",\n",
    "    include_system_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "eA0oMbemhbE-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': '## **Role & Objective**\\nYou are a reasoning expert specializing in **Chunked Symbolism**, a cognitive reasoning technique that organizes numerical reasoning into structured steps. Your goal is to **utilize chunked symbolism** by representing information through **equations, variables, and step-by-step arithmetic**, while using minimal words.\\n\\nChunked Symbolism is inspired by the cognitive science principle of **chunking**—the idea that humans process information more efficiently when grouped into meaningful units. Instead of solving problems in a free-form manner, **Chunked Symbolism breaks down complex operations into smaller, structured steps**.\\n\\nThis method is particularly effective for:\\n- **Mathematical problems** (arithmetic, algebra, physics, engineering)\\n- **Symbolic reasoning** (logic-based computations, formula derivations)\\n- **Technical calculations** (financial modeling, physics simulations, unit conversions)\\n\\n---\\n\\n## **How to Apply Chunked Symbolism**\\n### **Step-by-Step Guide**\\n1. **Identify Variables** – Extract relevant numerical values and define variables.\\n2. **Write Equations** – Represent the solution using **explicit mathematical formulas**.\\n3. **Perform Step-by-Step Computations** – Solve in **small, logical steps**, keeping each line clear.\\n4. **Label Units** – Maintain **consistent unit representation** to prevent ambiguity.\\n5. **Final Answer Formatting** – Present the answer in the **provided format** for clarity.\\n\\n---\\n\\n## **Rules & Directives**\\n1. **Use Equations & Variables**\\n   - Define variables before computation.\\n   - Always use **explicit equations** to represent reasoning.\\n\\n2. **Avoid Redundant Text**\\n   - **Do not** restate the problem; go directly to calculations.\\n   - Use **minimal context** only if it aids understanding.\\n\\n3. **Apply Step-by-Step Arithmetic**\\n   - Break operations into **small, structured steps**.\\n   - Ensure each line contains only **one computation** for clarity.\\n\\n4. **Output Format**\\n   - Use the exact structured format:\\n   ```\\n   <think>\\n   [shorthand reasoning]\\n   </think>\\n   \\\\boxed{[Final answer]}\\n   ```\\n   - The **final answer must be boxed**.\\n   - **If the question is multiple-choice, return the correct letter option inside the box.**\\n   - **Use minimal words in your response.**'},\n",
       " {'role': 'user',\n",
       "  'content': 'A car accelerates at 2.5 m/s^2 for 10 seconds. If its initial velocity was 15 m/s, what is its final velocity?'},\n",
       " {'role': 'assistant',\n",
       "  'content': '<think>\\na = 2.5 m/s^2\\nt = 10 s\\nvi = 15 m/s\\nvf = 15 + (2.5 × 10)\\nvf = 40 m/s\\n</think>\\n\\\\boxed{40}'},\n",
       " {'role': 'user',\n",
       "  'content': 'If a product costs $120 and there is a 15% discount, what is the final price?\\nChoices:\\nA) $10\\nB) $97\\nC) 102'},\n",
       " {'role': 'assistant',\n",
       "  'content': '<think>\\nop = 120\\nd = 15%\\ndp = 120 × (15 / 100) = 18\\nfp = 120 - 18 = 102\\n</think>\\n\\\\boxed{C}'},\n",
       " {'role': 'user',\n",
       "  'content': 'Question: A circuit has a voltage of 12V and a resistance of 4Ω. What is the current?'},\n",
       " {'role': 'assistant',\n",
       "  'content': '<think>\\nV = 12V\\nR = 4Ω\\nI = 12 / 4 = 3A\\n</think>\\n\\\\boxed{3}'},\n",
       " {'role': 'user',\n",
       "  'content': 'Alice has 5 apples. She gives 3 apples to Bob. How many apples does Alice have?'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "id": "J_DWSQGo_shV",
    "outputId": "bb07fe6b-7f2a-4d28-b846-75654da75a16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "A = 5\n",
      "A gives 3 to B: A = 5 - 3 = 2\n",
      "</think>\n",
      "\\boxed{2}\n"
     ]
    }
   ],
   "source": [
    "# Format for the model\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate response\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "# Decode response\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K0tIX_pVhiu9",
    "outputId": "d3f58a13-8638-422a-c339-ecf2bb0c3f34"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.11.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from faiss-cpu) (24.2)\n",
      "Downloading faiss_cpu-1.11.0-cp310-cp310-manylinux_2_28_x86_64.whl (31.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m208.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "F8Ax04MkLPMo"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional\n",
    "import faiss\n",
    "import json\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZN3guYplLKb1"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SoTExample:\n",
    "    \"\"\"Structure for SoT examples with metadata\"\"\"\n",
    "    question: str\n",
    "    answer: str\n",
    "    paradigm: str\n",
    "    domain: str\n",
    "    difficulty: str\n",
    "    keywords: List[str]\n",
    "    embedding: Optional[np.ndarray] = None\n",
    "\n",
    "class RAGEnhancedSoT:\n",
    "    \"\"\"SoT with RAG-based example retrieval\"\"\"\n",
    "\n",
    "    def __init__(self, embedding_model: str = \"all-MiniLM-L6-v2\", index_type: str = \"flat\"):\n",
    "        # Placeholder for SoT (assuming it's a class you have defined)\n",
    "        self.sot = SoT()\n",
    "        self.index_type = index_type\n",
    "        self.indices = {}\n",
    "        self.examples = {}\n",
    "        self.is_built = False\n",
    "\n",
    "        # Try SentenceTransformer, fall back to transformers if it fails\n",
    "        if SentenceTransformer is not None:\n",
    "            try:\n",
    "                self.embedding_model = SentenceTransformer(embedding_model)\n",
    "                self.model_type = \"sentence_transformer\"\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load SentenceTransformer: {e}\")\n",
    "                self._setup_fallback_model(embedding_model)\n",
    "        else:\n",
    "            print(\"SentenceTransformer not available, using fallback transformers model.\")\n",
    "            self._setup_fallback_model(embedding_model)\n",
    "\n",
    "    def _setup_fallback_model(self, model_name: str):\n",
    "        \"\"\"Fallback to Hugging Face transformers model if SentenceTransformer fails\"\"\"\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            self.model = AutoModel.from_pretrained(model_name)\n",
    "            self.model_type = \"transformers\"\n",
    "            self.model.eval()  # Set to evaluation mode\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to load fallback model {model_name}: {e}\")\n",
    "\n",
    "    def _encode(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Encode texts using either SentenceTransformer or transformers fallback\"\"\"\n",
    "        if self.model_type == \"sentence_transformer\":\n",
    "            return self.embedding_model.encode(texts)\n",
    "        else:\n",
    "            # Use transformers model with mean pooling\n",
    "            inputs = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            # Mean pooling of token embeddings for sentence representation\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "            return embeddings\n",
    "\n",
    "    def build_example_database(self, examples_by_paradigm: Dict[str, List[SoTExample]]):\n",
    "        \"\"\"Build FAISS indices for each paradigm\"\"\"\n",
    "        for paradigm, examples in examples_by_paradigm.items():\n",
    "            print(f\"Building index for {paradigm}...\")\n",
    "\n",
    "            # Generate embeddings for questions\n",
    "            questions = [ex.question for ex in examples]\n",
    "            embeddings = self._encode(questions)\n",
    "\n",
    "            # Store embeddings in examples\n",
    "            for i, example in enumerate(examples):\n",
    "                example.embedding = embeddings[i]\n",
    "\n",
    "            # Build FAISS index\n",
    "            dimension = embeddings.shape[1]\n",
    "            if self.index_type == \"flat\":\n",
    "                index = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity\n",
    "            else:\n",
    "                index = faiss.IndexHNSWFlat(dimension, 32)\n",
    "\n",
    "            # Normalize embeddings for cosine similarity\n",
    "            embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "            index.add(embeddings.astype('float32'))\n",
    "\n",
    "            self.indices[paradigm] = index\n",
    "            self.examples[paradigm] = examples\n",
    "\n",
    "        self.is_built = True\n",
    "        print(\"RAG indices built successfully!\")\n",
    "\n",
    "    def retrieve_relevant_examples(self,\n",
    "                                  query: str,\n",
    "                                  paradigm: str,\n",
    "                                  k: int = 3,\n",
    "                                  diversity_threshold: float = 0.7) -> List[SoTExample]:\n",
    "        \"\"\"Retrieve most relevant examples for a query\"\"\"\n",
    "        if not self.is_built:\n",
    "            raise ValueError(\"Must build example database first!\")\n",
    "\n",
    "        if paradigm not in self.indices:\n",
    "            raise ValueError(f\"Paradigm {paradigm} not found in indices\")\n",
    "\n",
    "        # Encode query\n",
    "        query_embedding = self._encode([query])\n",
    "        query_embedding = query_embedding / np.linalg.norm(query_embedding, axis=1, keepdims=True)\n",
    "\n",
    "        # Search for similar examples\n",
    "        scores, indices = self.indices[paradigm].search(\n",
    "            query_embedding.astype('float32'), k * 2  # Get more to allow for diversity filtering\n",
    "        )\n",
    "\n",
    "        # Retrieve examples and apply diversity filtering\n",
    "        candidates = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            if idx != -1:  # Valid index\n",
    "                example = self.examples[paradigm][idx]\n",
    "                candidates.append((example, score))\n",
    "\n",
    "        # Apply diversity filtering to avoid too similar examples\n",
    "        selected = []\n",
    "        for candidate, score in candidates:\n",
    "            if len(selected) >= k:\n",
    "                break\n",
    "\n",
    "            # Check diversity with already selected examples\n",
    "            is_diverse = True\n",
    "            for selected_ex, _ in selected:\n",
    "                similarity = np.dot(\n",
    "                    candidate.embedding,\n",
    "                    selected_ex.embedding\n",
    "                )\n",
    "                if similarity > diversity_threshold:\n",
    "                    is_diverse = False\n",
    "                    break\n",
    "\n",
    "            if is_diverse:\n",
    "                selected.append((candidate, score))\n",
    "\n",
    "        return [ex for ex, _ in selected]\n",
    "\n",
    "    def get_rag_enhanced_context(self,\n",
    "                               question: str,\n",
    "                               paradigm: Optional[str] = None,\n",
    "                               k_examples: int = 3,\n",
    "                               format: str = \"llm\",\n",
    "                               include_system_prompt: bool = True) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Get context with RAG-retrieved examples\"\"\"\n",
    "\n",
    "        # Auto-classify paradigm if not provided\n",
    "        if paradigm is None:\n",
    "            paradigm = self.sot.classify_question(question)\n",
    "\n",
    "        # Retrieve relevant examples\n",
    "        relevant_examples = self.retrieve_relevant_examples(\n",
    "            question, paradigm, k_examples\n",
    "        )\n",
    "\n",
    "        # Get system prompt for the paradigm\n",
    "        system_prompt = self.sot.get_system_prompt(paradigm)\n",
    "\n",
    "        # Build context messages\n",
    "        messages = []\n",
    "\n",
    "        if include_system_prompt:\n",
    "            messages.append({\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            })\n",
    "\n",
    "        # Add retrieved examples as few-shot demonstrations\n",
    "        for example in relevant_examples:\n",
    "            if format == \"llm\":\n",
    "                messages.extend([\n",
    "                    {\"role\": \"user\", \"content\": example.question},\n",
    "                    {\"role\": \"assistant\", \"content\": example.answer}\n",
    "                ])\n",
    "            elif format == \"vlm\":\n",
    "                messages.extend([\n",
    "                    {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": example.question}]},\n",
    "                    {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": example.answer}]}\n",
    "                ])\n",
    "\n",
    "        # Add the actual user question\n",
    "        if format == \"llm\":\n",
    "            messages.append({\"role\": \"user\", \"content\": question})\n",
    "        elif format == \"vlm\":\n",
    "            messages.append({\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": question}]})\n",
    "        elif format == \"raw\":\n",
    "            return [{\"question\": ex.question, \"answer\": ex.answer} for ex in relevant_examples]\n",
    "\n",
    "        return messages\n",
    "\n",
    "    def add_examples_from_interactions(self,\n",
    "                                     interactions: List[Dict[str, Any]]):\n",
    "        \"\"\"Add new examples from successful interactions\"\"\"\n",
    "        for interaction in interactions:\n",
    "            paradigm = interaction.get('paradigm')\n",
    "            if paradigm and paradigm in self.examples:\n",
    "                new_example = SoTExample(\n",
    "                    question=interaction['question'],\n",
    "                    answer=interaction['answer'],\n",
    "                    paradigm=paradigm,\n",
    "                    domain=interaction.get('domain', 'general'),\n",
    "                    difficulty=interaction.get('difficulty', 'medium'),\n",
    "                    keywords=interaction.get('keywords', [])\n",
    "                )\n",
    "\n",
    "                # Add to examples and rebuild index for this paradigm\n",
    "                self.examples[paradigm].append(new_example)\n",
    "                self._rebuild_paradigm_index(paradigm)\n",
    "\n",
    "    def _rebuild_paradigm_index(self, paradigm: str):\n",
    "        \"\"\"Rebuild index for a specific paradigm\"\"\"\n",
    "        if paradigm not in self.examples:\n",
    "            return\n",
    "\n",
    "        examples = self.examples[paradigm]\n",
    "        questions = [ex.question for ex in examples]\n",
    "        embeddings = self.embedding_model.encode(questions)\n",
    "\n",
    "        for i, example in enumerate(examples):\n",
    "            example.embedding = embeddings[i]\n",
    "\n",
    "        dimension = embeddings.shape[1]\n",
    "        if self.index_type == \"flat\":\n",
    "            index = faiss.IndexFlatIP(dimension)\n",
    "        else:\n",
    "            index = faiss.IndexHNSWFlat(dimension, 32)\n",
    "\n",
    "        embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "        index.add(embeddings.astype('float32'))\n",
    "\n",
    "        self.indices[paradigm] = index\n",
    "\n",
    "\n",
    "def load_examples_from_datasets(datasets_config: Dict[str, Any]) -> Dict[str, List[SoTExample]]:\n",
    "    \"\"\"Load and prepare examples from various datasets\"\"\"\n",
    "    examples_by_paradigm = {\n",
    "        'chunked_symbolism': [],\n",
    "        'conceptual_chaining': [],\n",
    "        'expert_lexicons': []\n",
    "    }\n",
    "\n",
    "    # Example loading logic - you'd adapt this for your datasets\n",
    "    for dataset_name, config in datasets_config.items():\n",
    "        # Load your dataset here\n",
    "        # Classify each example into appropriate paradigm\n",
    "        # Create SoTExample objects\n",
    "        pass\n",
    "\n",
    "    return examples_by_paradigm\n",
    "\n",
    "# Usage example with Qwen integration\n",
    "def main():\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "\n",
    "    # Initialize model\n",
    "    model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        config=config,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=None\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Initialize RAG-enhanced SoT\n",
    "    rag_sot = RAGEnhancedSoT()\n",
    "\n",
    "    # Create example database (you'd load this from your datasets)\n",
    "    examples_by_paradigm = {\n",
    "        'chunked_symbolism': [\n",
    "            SoTExample(\n",
    "                question=\"Alice has 5 apples. She gives 3 to Bob. How many does she have left?\",\n",
    "                answer=\"<think>\\nA = 5\\nA -= 3\\nA = 2\\n</think>\\n\\\\boxed{2}\",\n",
    "                paradigm='chunked_symbolism',\n",
    "                domain='arithmetic',\n",
    "                difficulty='easy',\n",
    "                keywords=['subtraction', 'word_problem']\n",
    "            ),\n",
    "            SoTExample(\n",
    "                question=\"A store has 24 books. They sell 8 books and get 15 new ones. How many books are there now?\",\n",
    "                answer=\"<think>\\nB = 24\\nB -= 8\\nB = 16\\nB += 15\\nB = 31\\n</think>\\n\\\\boxed{31}\",\n",
    "                paradigm='chunked_symbolism',\n",
    "                domain='arithmetic',\n",
    "                difficulty='easy',\n",
    "                keywords=['addition', 'subtraction', 'word_problem']\n",
    "            ),\n",
    "            SoTExample(\n",
    "                question=\"If a rectangle has length 8 and width 5, what is its area?\",\n",
    "                answer=\"<think>\\nL = 8\\nW = 5\\nArea = L × W = 8 × 5 = 40\\n</think>\\n\\\\boxed{40}\",\n",
    "                paradigm='chunked_symbolism',\n",
    "                domain='geometry',\n",
    "                difficulty='easy',\n",
    "                keywords=['rectangle', 'area', 'multiplication']\n",
    "            ),\n",
    "        ],\n",
    "        'conceptual_chaining': [\n",
    "            SoTExample(\n",
    "                question=\"Why do birds migrate south for winter?\",\n",
    "                answer=\"<think>\\nCold→Food scarcity→Survival instinct→Migration→Warmer climate→Food availability\\n</think>\\nBirds migrate south because winter brings cold temperatures that reduce food availability, triggering their survival instinct to seek warmer climates where food is more abundant.\",\n",
    "                paradigm='conceptual_chaining',\n",
    "                domain='biology',\n",
    "                difficulty='medium',\n",
    "                keywords=['migration', 'survival', 'climate']\n",
    "            ),\n",
    "            SoTExample(\n",
    "                question=\"How does exercise benefit mental health?\",\n",
    "                answer=\"<think>\\nExercise→Endorphin release→Mood improvement→Stress reduction→Better sleep→Enhanced self-esteem\\n</think>\\nExercise benefits mental health by releasing endorphins that improve mood, reducing stress hormones, promoting better sleep patterns, and boosting self-esteem through physical achievement.\",\n",
    "                paradigm='conceptual_chaining',\n",
    "                domain='health',\n",
    "                difficulty='medium',\n",
    "                keywords=['exercise', 'mental_health', 'endorphins']\n",
    "            ),\n",
    "        ],\n",
    "        'expert_lexicons': [\n",
    "            SoTExample(\n",
    "                question=\"What happens during TCP three-way handshake?\",\n",
    "                answer=\"<think>\\nSYN→SYN-ACK→ACK | seq/ack nums | connection established\\n</think>\\nTCP 3-way handshake: Client sends SYN, server responds SYN-ACK, client sends ACK. Establishes bidirectional connection with synchronized sequence numbers.\",\n",
    "                paradigm='expert_lexicons',\n",
    "                domain='networking',\n",
    "                difficulty='hard',\n",
    "                keywords=['TCP', 'handshake', 'protocol']\n",
    "            ),\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Build the RAG database\n",
    "    rag_sot.build_example_database(examples_by_paradigm)\n",
    "\n",
    "    # Test question\n",
    "    prompt = \"Alice has 5 apples. She gives 3 apples to Bob. How many apples does Alice have?\"\n",
    "\n",
    "    print(\"=== RAG-Enhanced SoT with Qwen ===\\n\")\n",
    "\n",
    "    # Get RAG-enhanced context (instead of regular SoT)\n",
    "    messages = rag_sot.get_rag_enhanced_context(\n",
    "        question=prompt,\n",
    "        k_examples=2,  # Retrieve 2 most relevant examples\n",
    "        format=\"llm\",\n",
    "        include_system_prompt=True\n",
    "    )\n",
    "\n",
    "    print(\"Retrieved examples for context:\")\n",
    "    for i, msg in enumerate(messages):\n",
    "        if msg['role'] == 'user' and i > 0:  # Skip system prompt\n",
    "            print(f\"Example Q: {msg['content'][:80]}...\")\n",
    "        elif msg['role'] == 'assistant':\n",
    "            print(f\"Example A: {msg['content'][:80]}...\")\n",
    "\n",
    "    print(f\"\\nActual question: {prompt}\\n\")\n",
    "\n",
    "    # Format for the model\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate response\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.1,\n",
    "        do_sample=True\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    # Decode response\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    print(\"RAG-Enhanced SoT Response:\")\n",
    "    print(response)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "    # Compare with standard SoT\n",
    "    print(\"\\n=== Standard SoT Comparison ===\\n\")\n",
    "    sot = SoT()\n",
    "    paradigm = sot.classify_question(prompt)\n",
    "    standard_messages = sot.get_initialized_context(\n",
    "        paradigm,\n",
    "        prompt,\n",
    "        format=\"llm\",\n",
    "        include_system_prompt=True\n",
    "    )\n",
    "\n",
    "    # Generate with standard SoT\n",
    "    standard_text = tokenizer.apply_chat_template(\n",
    "        standard_messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    standard_inputs = tokenizer([standard_text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    standard_generated = model.generate(\n",
    "        **standard_inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.1,\n",
    "        do_sample=True\n",
    "    )\n",
    "    standard_generated = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(standard_inputs.input_ids, standard_generated)\n",
    "    ]\n",
    "\n",
    "    standard_response = tokenizer.batch_decode(standard_generated, skip_special_tokens=True)[0]\n",
    "    print(\"Standard SoT Response:\")\n",
    "    print(standard_response)\n",
    "\n",
    "def demo_different_question_types():\n",
    "    \"\"\"Demo RAG-SoT with different types of questions\"\"\"\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "\n",
    "    # Initialize components (same as above)\n",
    "    model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        config=config,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    rag_sot = RAGEnhancedSoT()\n",
    "\n",
    "    # Same example database as above...\n",
    "    # (Build database code here)\n",
    "\n",
    "    # Test different question types\n",
    "    test_questions = [\n",
    "        \"John has 10 cookies and eats 3. How many are left?\",  # Should get chunked_symbolism examples\n",
    "        \"Why do plants need sunlight to grow?\",  # Should get conceptual_chaining examples\n",
    "        \"Explain how HTTP status codes work\",  # Should get expert_lexicons examples\n",
    "    ]\n",
    "\n",
    "    for question in test_questions:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Question: {question}\")\n",
    "\n",
    "        # Get paradigm and relevant examples\n",
    "        paradigm = rag_sot.sot.classify_question(question)\n",
    "        print(f\"Detected paradigm: {paradigm}\")\n",
    "\n",
    "        relevant_examples = rag_sot.retrieve_relevant_examples(question, paradigm, k=2)\n",
    "        print(f\"Retrieved {len(relevant_examples)} relevant examples\")\n",
    "\n",
    "        # Generate response\n",
    "        messages = rag_sot.get_rag_enhanced_context(\n",
    "            question=question,\n",
    "            k_examples=2,\n",
    "            format=\"llm\"\n",
    "        )\n",
    "\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        generated_ids = model.generate(**model_inputs, max_new_tokens=256, temperature=0.1)\n",
    "        generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n",
    "\n",
    "        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        print(f\"Response: {response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    # Uncomment to run demo with different question types\n",
    "    # demo_different_question_types()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate) (2.4.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate) (0.32.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2025.5.1)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (1.1.3)\n",
      "Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->accelerate) (12.8.61)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c13fbf635f24050ae5c6c0f5f98c291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building index for chunked_symbolism...\n",
      "Building index for conceptual_chaining...\n",
      "Building index for expert_lexicons...\n",
      "RAG indices built successfully!\n",
      "=== RAG-Enhanced SoT with Qwen ===\n",
      "\n",
      "Retrieved examples for context:\n",
      "Example Q: Alice has 5 apples. She gives 3 to Bob. How many does she have left?...\n",
      "Example A: <think>\n",
      "A = 5\n",
      "A -= 3\n",
      "A = 2\n",
      "</think>\n",
      "\\boxed{2}...\n",
      "Example Q: A store has 24 books. They sell 8 books and get 15 new ones. How many books are ...\n",
      "Example A: <think>\n",
      "B = 24\n",
      "B -= 8\n",
      "B = 16\n",
      "B += 15\n",
      "B = 31\n",
      "</think>\n",
      "\\boxed{31}...\n",
      "Example Q: Alice has 5 apples. She gives 3 apples to Bob. How many apples does Alice have?...\n",
      "\n",
      "Actual question: Alice has 5 apples. She gives 3 apples to Bob. How many apples does Alice have?\n",
      "\n",
      "RAG-Enhanced SoT Response:\n",
      "<think>\n",
      "A = 5\n",
      "A -= 3\n",
      "A = 2\n",
      "</think>\n",
      "\\boxed{2}\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== Standard SoT Comparison ===\n",
      "\n",
      "Standard SoT Response:\n",
      "<think>\n",
      "A = 5\n",
      "A -= 3\n",
      "A = 2\n",
      "</think>\n",
      "\\boxed{2}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "658ad95d2bf44170aabdd92649fe4e84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building index for chunked_symbolism...\n",
      "Building index for conceptual_chaining...\n",
      "Building index for expert_lexicons...\n",
      "RAG indices built successfully!\n",
      "\n",
      "============================================================\n",
      "Question: John has 10 cookies and eats 3. How many are left?\n",
      "Detected paradigm: chunked_symbolism\n",
      "Retrieved 2 relevant examples\n",
      "Retrieved examples for context:\n",
      "Example 1 Q: Alice has 5 apples. She gives 3 to Bob. How many does she have left?...\n",
      "Example 1 A: <think>\n",
      "A = 5\n",
      "A -= 3\n",
      "A = 2\n",
      "</think>\n",
      "\\boxed{2}...\n",
      "Example 2 Q: A store has 24 books. They sell 8 books and get 15 new ones. How many books are ...\n",
      "Example 2 A: <think>\n",
      "B = 24\n",
      "B -= 8\n",
      "B = 16\n",
      "B += 15\n",
      "B = 31\n",
      "</think>\n",
      "\\boxed{31}...\n",
      "Response: <think>\n",
      "C = 10\n",
      "C -= 3\n",
      "C = 7\n",
      "</think>\n",
      "\\boxed{7}\n",
      "\n",
      "============================================================\n",
      "Question: Why do plants need sunlight to grow?\n",
      "Detected paradigm: conceptual_chaining\n",
      "Retrieved 2 relevant examples\n",
      "Retrieved examples for context:\n",
      "Example 1 Q: Why do birds migrate south for winter?...\n",
      "Example 1 A: <think>\n",
      "Cold→Food scarcity→Survival instinct→Migration→Warmer climate→Food avail...\n",
      "Example 2 Q: How does exercise benefit mental health?...\n",
      "Example 2 A: <think>\n",
      "Exercise→Endorphin release→Mood improvement→Stress reduction→Better slee...\n",
      "Response: <think>\n",
      "Sunlight→Photosynthesis→Glucose production→Energy source→Growth\n",
      "</think>\n",
      "Plants need sunlight to grow because sunlight is essential for photosynthesis, which produces glucose as an energy source for growth.\n",
      "\n",
      "============================================================\n",
      "Question: Explain how HTTP status codes work\n",
      "Detected paradigm: conceptual_chaining\n",
      "Retrieved 2 relevant examples\n",
      "Retrieved examples for context:\n",
      "Example 1 Q: How does exercise benefit mental health?...\n",
      "Example 1 A: <think>\n",
      "Exercise→Endorphin release→Mood improvement→Stress reduction→Better slee...\n",
      "Example 2 Q: Why do birds migrate south for winter?...\n",
      "Example 2 A: <think>\n",
      "Cold→Food scarcity→Survival instinct→Migration→Warmer climate→Food avail...\n",
      "Response: <think>\n",
      "HTTP request→Server response→Status code range→2xx Success→3xx Redirection→4xx Client error→5xx Server error\n",
      "</think>\n",
      "HTTP status codes work by indicating the result of an HTTP request. They fall into five ranges: 2xx for success, 3xx for redirection, 4xx for client errors, and 5xx for server errors.\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Dict, Any\n",
    "import numpy as np\n",
    "import torch\n",
    "import faiss\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SoTExample:\n",
    "    \"\"\"Structure for SoT examples with metadata\"\"\"\n",
    "    question: str\n",
    "    answer: str\n",
    "    paradigm: str\n",
    "    domain: str\n",
    "    difficulty: str\n",
    "    keywords: List[str]\n",
    "    embedding: Optional[np.ndarray] = None\n",
    "\n",
    "\n",
    "class RAGEnhancedSoT:\n",
    "    \"\"\"SoT with RAG-based example retrieval using Hugging Face transformers embeddings\"\"\"\n",
    "\n",
    "    def __init__(self, embedding_model: str = \"sentence-transformers/all-MiniLM-L6-v2\", index_type: str = \"flat\"):\n",
    "        self.sot = SoT()  # Your SoT class\n",
    "        self.index_type = index_type\n",
    "        self.indices = {}\n",
    "        self.examples = {}\n",
    "        self.is_built = False\n",
    "\n",
    "        # Load tokenizer and model from HF\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(embedding_model)\n",
    "        self.model = AutoModel.from_pretrained(embedding_model)\n",
    "        self.model.eval()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def _encode(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Encode texts using transformers model with mean pooling\"\"\"\n",
    "        inputs = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "\n",
    "        # Mean pooling (ignore padding tokens)\n",
    "        attention_mask = inputs['attention_mask'].unsqueeze(-1)\n",
    "        token_embeddings = outputs.last_hidden_state  # (batch, seq_len, hidden)\n",
    "        masked_embeddings = token_embeddings * attention_mask\n",
    "        summed = masked_embeddings.sum(dim=1)\n",
    "        counts = attention_mask.sum(dim=1).clamp(min=1e-9)\n",
    "        mean_pooled = summed / counts\n",
    "\n",
    "        return mean_pooled.cpu().numpy()\n",
    "\n",
    "    def build_example_database(self, examples_by_paradigm: Dict[str, List[SoTExample]]):\n",
    "        \"\"\"Build FAISS indices for each paradigm\"\"\"\n",
    "        for paradigm, examples in examples_by_paradigm.items():\n",
    "            print(f\"Building index for {paradigm}...\")\n",
    "\n",
    "            questions = [ex.question for ex in examples]\n",
    "            embeddings = self._encode(questions)\n",
    "\n",
    "            for i, example in enumerate(examples):\n",
    "                example.embedding = embeddings[i]\n",
    "\n",
    "            dim = embeddings.shape[1]\n",
    "            if self.index_type == \"flat\":\n",
    "                index = faiss.IndexFlatIP(dim)\n",
    "            else:\n",
    "                index = faiss.IndexHNSWFlat(dim, 32)\n",
    "\n",
    "            # Normalize embeddings for cosine similarity\n",
    "            faiss.normalize_L2(embeddings)\n",
    "            index.add(embeddings.astype('float32'))\n",
    "\n",
    "            self.indices[paradigm] = index\n",
    "            self.examples[paradigm] = examples\n",
    "\n",
    "        self.is_built = True\n",
    "        print(\"RAG indices built successfully!\")\n",
    "\n",
    "    def retrieve_relevant_examples(self,\n",
    "                                  query: str,\n",
    "                                  paradigm: str,\n",
    "                                  k: int = 3,\n",
    "                                  diversity_threshold: float = 0.7) -> List[SoTExample]:\n",
    "        \"\"\"Retrieve most relevant examples for a query with diversity filtering\"\"\"\n",
    "\n",
    "        if not self.is_built:\n",
    "            raise ValueError(\"Example database not built yet.\")\n",
    "        if paradigm not in self.indices:\n",
    "            raise ValueError(f\"Paradigm '{paradigm}' not found in indices.\")\n",
    "\n",
    "        query_emb = self._encode([query])\n",
    "        faiss.normalize_L2(query_emb)\n",
    "\n",
    "        scores, indices = self.indices[paradigm].search(query_emb.astype('float32'), k * 3)  # Over-fetch for diversity\n",
    "\n",
    "        candidates = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            if idx == -1:\n",
    "                continue\n",
    "            example = self.examples[paradigm][idx]\n",
    "            candidates.append((example, score))\n",
    "\n",
    "        selected = []\n",
    "        for candidate, score in candidates:\n",
    "            if len(selected) >= k:\n",
    "                break\n",
    "            is_diverse = True\n",
    "            for sel_ex, _ in selected:\n",
    "                sim = np.dot(candidate.embedding, sel_ex.embedding)\n",
    "                if sim > diversity_threshold:\n",
    "                    is_diverse = False\n",
    "                    break\n",
    "            if is_diverse:\n",
    "                selected.append((candidate, score))\n",
    "\n",
    "        return [ex for ex, _ in selected]\n",
    "\n",
    "    def get_rag_enhanced_context(self,\n",
    "                                 question: str,\n",
    "                                 paradigm: Optional[str] = None,\n",
    "                                 k_examples: int = 3,\n",
    "                                 format: str = \"llm\",\n",
    "                                 include_system_prompt: bool = True) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Get context with RAG-retrieved examples for prompt construction\"\"\"\n",
    "\n",
    "        if paradigm is None:\n",
    "            paradigm = self.sot.classify_question(question)\n",
    "\n",
    "        relevant_examples = self.retrieve_relevant_examples(question, paradigm, k=k_examples)\n",
    "\n",
    "        system_prompt = self.sot.get_system_prompt(paradigm)\n",
    "\n",
    "        messages = []\n",
    "\n",
    "        if include_system_prompt:\n",
    "            messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "\n",
    "        for example in relevant_examples:\n",
    "            if format == \"llm\":\n",
    "                messages.append({\"role\": \"user\", \"content\": example.question})\n",
    "                messages.append({\"role\": \"assistant\", \"content\": example.answer})\n",
    "            elif format == \"vlm\":\n",
    "                messages.append({\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": example.question}]})\n",
    "                messages.append({\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": example.answer}]})\n",
    "\n",
    "        if format == \"llm\":\n",
    "            messages.append({\"role\": \"user\", \"content\": question})\n",
    "        elif format == \"vlm\":\n",
    "            messages.append({\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": question}]})\n",
    "        elif format == \"raw\":\n",
    "            return [{\"question\": ex.question, \"answer\": ex.answer} for ex in relevant_examples]\n",
    "\n",
    "        return messages\n",
    "\n",
    "    def add_examples_from_interactions(self, interactions: List[Dict[str, Any]]):\n",
    "        \"\"\"Add new examples and rebuild indices dynamically\"\"\"\n",
    "\n",
    "        for interaction in interactions:\n",
    "            paradigm = interaction.get(\"paradigm\")\n",
    "            if paradigm and paradigm in self.examples:\n",
    "                new_example = SoTExample(\n",
    "                    question=interaction[\"question\"],\n",
    "                    answer=interaction[\"answer\"],\n",
    "                    paradigm=paradigm,\n",
    "                    domain=interaction.get(\"domain\", \"general\"),\n",
    "                    difficulty=interaction.get(\"difficulty\", \"medium\"),\n",
    "                    keywords=interaction.get(\"keywords\", []),\n",
    "                )\n",
    "                self.examples[paradigm].append(new_example)\n",
    "                self._rebuild_paradigm_index(paradigm)\n",
    "\n",
    "    def _rebuild_paradigm_index(self, paradigm: str):\n",
    "        \"\"\"Rebuild index for a given paradigm after adding new examples\"\"\"\n",
    "\n",
    "        examples = self.examples.get(paradigm, [])\n",
    "        if not examples:\n",
    "            return\n",
    "\n",
    "        questions = [ex.question for ex in examples]\n",
    "        embeddings = self._encode(questions)\n",
    "\n",
    "        for i, example in enumerate(examples):\n",
    "            example.embedding = embeddings[i]\n",
    "\n",
    "        dim = embeddings.shape[1]\n",
    "        if self.index_type == \"flat\":\n",
    "            index = faiss.IndexFlatIP(dim)\n",
    "        else:\n",
    "            index = faiss.IndexHNSWFlat(dim, 32)\n",
    "\n",
    "        faiss.normalize_L2(embeddings)\n",
    "        index.add(embeddings.astype('float32'))\n",
    "\n",
    "        self.indices[paradigm] = index\n",
    "\n",
    "\n",
    "\n",
    "def load_examples_from_datasets(datasets_config: Dict[str, Any]) -> Dict[str, List[SoTExample]]:\n",
    "    \"\"\"Load and prepare examples from various datasets\"\"\"\n",
    "    examples_by_paradigm = {\n",
    "        'chunked_symbolism': [],\n",
    "        'conceptual_chaining': [],\n",
    "        'expert_lexicons': []\n",
    "    }\n",
    "\n",
    "    # Example loading logic - you'd adapt this for your datasets\n",
    "    for dataset_name, config in datasets_config.items():\n",
    "        # Load your dataset here\n",
    "        # Classify each example into appropriate paradigm\n",
    "        # Create SoTExample objects\n",
    "        pass\n",
    "\n",
    "    return examples_by_paradigm\n",
    "\n",
    "# Usage example with Qwen integration\n",
    "def main():\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "\n",
    "    # Initialize model\n",
    "    model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        config=config,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=None\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Initialize RAG-enhanced SoT\n",
    "    rag_sot = RAGEnhancedSoT()\n",
    "\n",
    "    # Create example database (you'd load this from your datasets)\n",
    "    examples_by_paradigm = {\n",
    "        'chunked_symbolism': [\n",
    "            SoTExample(\n",
    "                question=\"Alice has 5 apples. She gives 3 to Bob. How many does she have left?\",\n",
    "                answer=\"<think>\\nA = 5\\nA -= 3\\nA = 2\\n</think>\\n\\\\boxed{2}\",\n",
    "                paradigm='chunked_symbolism',\n",
    "                domain='arithmetic',\n",
    "                difficulty='easy',\n",
    "                keywords=['subtraction', 'word_problem']\n",
    "            ),\n",
    "            SoTExample(\n",
    "                question=\"A store has 24 books. They sell 8 books and get 15 new ones. How many books are there now?\",\n",
    "                answer=\"<think>\\nB = 24\\nB -= 8\\nB = 16\\nB += 15\\nB = 31\\n</think>\\n\\\\boxed{31}\",\n",
    "                paradigm='chunked_symbolism',\n",
    "                domain='arithmetic',\n",
    "                difficulty='easy',\n",
    "                keywords=['addition', 'subtraction', 'word_problem']\n",
    "            ),\n",
    "            SoTExample(\n",
    "                question=\"If a rectangle has length 8 and width 5, what is its area?\",\n",
    "                answer=\"<think>\\nL = 8\\nW = 5\\nArea = L × W = 8 × 5 = 40\\n</think>\\n\\\\boxed{40}\",\n",
    "                paradigm='chunked_symbolism',\n",
    "                domain='geometry',\n",
    "                difficulty='easy',\n",
    "                keywords=['rectangle', 'area', 'multiplication']\n",
    "            ),\n",
    "        ],\n",
    "        'conceptual_chaining': [\n",
    "            SoTExample(\n",
    "                question=\"Why do birds migrate south for winter?\",\n",
    "                answer=\"<think>\\nCold→Food scarcity→Survival instinct→Migration→Warmer climate→Food availability\\n</think>\\nBirds migrate south because winter brings cold temperatures that reduce food availability, triggering their survival instinct to seek warmer climates where food is more abundant.\",\n",
    "                paradigm='conceptual_chaining',\n",
    "                domain='biology',\n",
    "                difficulty='medium',\n",
    "                keywords=['migration', 'survival', 'climate']\n",
    "            ),\n",
    "            SoTExample(\n",
    "                question=\"How does exercise benefit mental health?\",\n",
    "                answer=\"<think>\\nExercise→Endorphin release→Mood improvement→Stress reduction→Better sleep→Enhanced self-esteem\\n</think>\\nExercise benefits mental health by releasing endorphins that improve mood, reducing stress hormones, promoting better sleep patterns, and boosting self-esteem through physical achievement.\",\n",
    "                paradigm='conceptual_chaining',\n",
    "                domain='health',\n",
    "                difficulty='medium',\n",
    "                keywords=['exercise', 'mental_health', 'endorphins']\n",
    "            ),\n",
    "        ],\n",
    "        'expert_lexicons': [\n",
    "            SoTExample(\n",
    "                question=\"What happens during TCP three-way handshake?\",\n",
    "                answer=\"<think>\\nSYN→SYN-ACK→ACK | seq/ack nums | connection established\\n</think>\\nTCP 3-way handshake: Client sends SYN, server responds SYN-ACK, client sends ACK. Establishes bidirectional connection with synchronized sequence numbers.\",\n",
    "                paradigm='expert_lexicons',\n",
    "                domain='networking',\n",
    "                difficulty='hard',\n",
    "                keywords=['TCP', 'handshake', 'protocol']\n",
    "            ),\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Build the RAG database\n",
    "    rag_sot.build_example_database(examples_by_paradigm)\n",
    "\n",
    "    # Test question\n",
    "    prompt = \"Alice has 5 apples. She gives 3 apples to Bob. How many apples does Alice have?\"\n",
    "\n",
    "    print(\"=== RAG-Enhanced SoT with Qwen ===\\n\")\n",
    "\n",
    "    # Get RAG-enhanced context (instead of regular SoT)\n",
    "    messages = rag_sot.get_rag_enhanced_context(\n",
    "        question=prompt,\n",
    "        k_examples=2,  # Retrieve 2 most relevant examples\n",
    "        format=\"llm\",\n",
    "        include_system_prompt=True\n",
    "    )\n",
    "\n",
    "    print(\"Retrieved examples for context:\")\n",
    "    for i, msg in enumerate(messages):\n",
    "        if msg['role'] == 'user' and i > 0:  # Skip system prompt\n",
    "            print(f\"Example Q: {msg['content'][:80]}...\")\n",
    "        elif msg['role'] == 'assistant':\n",
    "            print(f\"Example A: {msg['content'][:80]}...\")\n",
    "\n",
    "    print(f\"\\nActual question: {prompt}\\n\")\n",
    "\n",
    "    # Format for the model\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate response\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.1,\n",
    "        do_sample=True\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    # Decode response\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    print(\"RAG-Enhanced SoT Response:\")\n",
    "    print(response)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "    # Compare with standard SoT\n",
    "    print(\"\\n=== Standard SoT Comparison ===\\n\")\n",
    "    sot = SoT()\n",
    "    paradigm = sot.classify_question(prompt)\n",
    "    standard_messages = sot.get_initialized_context(\n",
    "        paradigm,\n",
    "        prompt,\n",
    "        format=\"llm\",\n",
    "        include_system_prompt=True\n",
    "    )\n",
    "\n",
    "    # Generate with standard SoT\n",
    "    standard_text = tokenizer.apply_chat_template(\n",
    "        standard_messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    standard_inputs = tokenizer([standard_text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    standard_generated = model.generate(\n",
    "        **standard_inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.1,\n",
    "        do_sample=True\n",
    "    )\n",
    "    standard_generated = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(standard_inputs.input_ids, standard_generated)\n",
    "    ]\n",
    "\n",
    "    standard_response = tokenizer.batch_decode(standard_generated, skip_special_tokens=True)[0]\n",
    "    print(\"Standard SoT Response:\")\n",
    "    print(standard_response)\n",
    "\n",
    "def demo_different_question_types():\n",
    "    \"\"\"Demo RAG-SoT with different types of questions\"\"\"\n",
    "    # Initialize components\n",
    "    model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "    try:\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            config=config,\n",
    "            torch_dtype=\"auto\",\n",
    "            device_map=None  # Requires 'accelerate' installed\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load model or tokenizer: {e}\")\n",
    "        return\n",
    "\n",
    "    rag_sot = RAGEnhancedSoT()\n",
    "\n",
    "    # Example database\n",
    "    examples_by_paradigm = {\n",
    "        'chunked_symbolism': [\n",
    "            SoTExample(\n",
    "                question=\"Alice has 5 apples. She gives 3 to Bob. How many does she have left?\",\n",
    "                answer=\"<think>\\nA = 5\\nA -= 3\\nA = 2\\n</think>\\n\\\\boxed{2}\",\n",
    "                paradigm='chunked_symbolism',\n",
    "                domain='arithmetic',\n",
    "                difficulty='easy',\n",
    "                keywords=['subtraction', 'word_problem']\n",
    "            ),\n",
    "            SoTExample(\n",
    "                question=\"A store has 24 books. They sell 8 books and get 15 new ones. How many books are there now?\",\n",
    "                answer=\"<think>\\nB = 24\\nB -= 8\\nB = 16\\nB += 15\\nB = 31\\n</think>\\n\\\\boxed{31}\",\n",
    "                paradigm='chunked_symbolism',\n",
    "                domain='arithmetic',\n",
    "                difficulty='easy',\n",
    "                keywords=['addition', 'subtraction', 'word_problem']\n",
    "            ),\n",
    "            SoTExample(\n",
    "                question=\"If a rectangle has length 8 and width 5, what is its area?\",\n",
    "                answer=\"<think>\\nL = 8\\nW = 5\\nArea = L × W = 8 × 5 = 40\\n</think>\\n\\\\boxed{40}\",\n",
    "                paradigm='chunked_symbolism',\n",
    "                domain='geometry',\n",
    "                difficulty='easy',\n",
    "                keywords=['rectangle', 'area', 'multiplication']\n",
    "            ),\n",
    "        ],\n",
    "        'conceptual_chaining': [\n",
    "            SoTExample(\n",
    "                question=\"Why do birds migrate south for winter?\",\n",
    "                answer=\"<think>\\nCold→Food scarcity→Survival instinct→Migration→Warmer climate→Food availability\\n</think>\\nBirds migrate south because winter brings cold temperatures that reduce food availability, triggering their survival instinct to seek warmer climates where food is more abundant.\",\n",
    "                paradigm='conceptual_chaining',\n",
    "                domain='biology',\n",
    "                difficulty='medium',\n",
    "                keywords=['migration', 'survival', 'climate']\n",
    "            ),\n",
    "            SoTExample(\n",
    "                question=\"How does exercise benefit mental health?\",\n",
    "                answer=\"<think>\\nExercise→Endorphin release→Mood improvement→Stress reduction→Better sleep→Enhanced self-esteem\\n</think>\\nExercise benefits mental health by releasing endorphins that improve mood, reducing stress hormones, promoting better sleep patterns, and boosting self-esteem through physical achievement.\",\n",
    "                paradigm='conceptual_chaining',\n",
    "                domain='health',\n",
    "                difficulty='medium',\n",
    "                keywords=['exercise', 'mental_health', 'endorphins']\n",
    "            ),\n",
    "        ],\n",
    "        'expert_lexicons': [\n",
    "            SoTExample(\n",
    "                question=\"What happens during TCP three-way handshake?\",\n",
    "                answer=\"<think>\\nSYN→SYN-ACK→ACK | seq/ack nums | connection established\\n</think>\\nTCP 3-way handshake: Client sends SYN, server responds SYN-ACK, client sends ACK. Establishes bidirectional connection with synchronized sequence numbers.\",\n",
    "                paradigm='expert_lexicons',\n",
    "                domain='networking',\n",
    "                difficulty='hard',\n",
    "                keywords=['TCP', 'handshake', 'protocol']\n",
    "            ),\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Build the example database\n",
    "    try:\n",
    "        rag_sot.build_example_database(examples_by_paradigm)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to build example database: {e}\")\n",
    "        return\n",
    "\n",
    "    # Test different question types\n",
    "    test_questions = [\n",
    "        \"John has 10 cookies and eats 3. How many are left?\",\n",
    "        \"Why do plants need sunlight to grow?\",\n",
    "        \"Explain how HTTP status codes work\"\n",
    "    ]\n",
    "\n",
    "    for question in test_questions:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Question: {question}\")\n",
    "        \n",
    "        # Get paradigm and relevant examples\n",
    "        try:\n",
    "            paradigm = rag_sot.sot.classify_question(question)\n",
    "            print(f\"Detected paradigm: {paradigm}\")\n",
    "            \n",
    "            relevant_examples = rag_sot.retrieve_relevant_examples(question, paradigm, k=2)\n",
    "            print(f\"Retrieved {len(relevant_examples)} relevant examples\")\n",
    "            \n",
    "            # Print retrieved examples\n",
    "            print(\"Retrieved examples for context:\")\n",
    "            for i, example in enumerate(relevant_examples, 1):\n",
    "                print(f\"Example {i} Q: {example.question[:80]}...\")\n",
    "                print(f\"Example {i} A: {example.answer[:80]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving examples for '{question}': {e}\")\n",
    "            continue\n",
    "\n",
    "        # Generate response\n",
    "        try:\n",
    "            messages = rag_sot.get_rag_enhanced_context(\n",
    "                question=question,\n",
    "                k_examples=2,\n",
    "                format=\"llm\"\n",
    "            )\n",
    "            text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "            model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "            generated_ids = model.generate(**model_inputs, max_new_tokens=256, temperature=0.1)\n",
    "            generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n",
    "            response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "            print(f\"Response: {response}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question '{question}': {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    # Uncomment to run demo with different question types\n",
    "    demo_different_question_types()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "043fc5ff138f47a185c9f1eff3449fbd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "05349c36cd2f4f9d9e9f14591adf1ff2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0610812e4f764ad3b7c70e691042ff9e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "08d06d1cbe0e4dae9ec7704b87352b50": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "092488bad9874ad291c0581dfd6e8c75": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0e5d4fad0a864687a4a8c5431f3e25fe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0edc94621a93495a82b3f794f22dc088": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_aa9da7977aa54c2fbc186f05b99b2b4b",
       "IPY_MODEL_1f22d499d24d4f958d3e9b01c11fed26",
       "IPY_MODEL_559f7830897a4717a80792c981a23140"
      ],
      "layout": "IPY_MODEL_0e5d4fad0a864687a4a8c5431f3e25fe"
     }
    },
    "137f65ad3b414cd2856f658be634d375": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fc77af29a9bc4f199a60501250d1c08f",
      "max": 3945441440,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_752a926ef8b64cf58ba7ef768c13e0e9",
      "value": 3945441440
     }
    },
    "1e18fb576c5341a797f2976b93083567": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9f5c1db60551497897fe4c4d1d7521d6",
      "placeholder": "​",
      "style": "IPY_MODEL_3bb6fb8571c2426480acfd4ab9f78946",
      "value": " 27.8k/27.8k [00:00&lt;00:00, 2.41MB/s]"
     }
    },
    "1f22d499d24d4f958d3e9b01c11fed26": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_40f585769c6b439790f3958ed8cb5c8e",
      "max": 663,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bfaac2bc1cb84cd4b0a281eece1dcbd0",
      "value": 663
     }
    },
    "20f277ccf4bb4ea1b20b761b1b4ba6c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "22531ce7b67d4567acf1c967de042e27": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "27972f5fc42c40978a16eaa4040cae6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_52ce361d1d0f401aa6f55d01726afad2",
       "IPY_MODEL_cd49d122f5404e1e9574774d1bb3af7b",
       "IPY_MODEL_f5b4cffdaf4f42788c90294ec95d3984"
      ],
      "layout": "IPY_MODEL_db360d49a6b245369229d963a729c87d"
     }
    },
    "280d17af15684a5797a9f34f1c6c00d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_092488bad9874ad291c0581dfd6e8c75",
      "placeholder": "​",
      "style": "IPY_MODEL_20f277ccf4bb4ea1b20b761b1b4ba6c6",
      "value": " 3.95G/3.95G [01:07&lt;00:00, 32.2MB/s]"
     }
    },
    "287bf084dadb4fc28f4e12290550135d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ed60e5c0420c43a29935b1f0f98d73cd",
       "IPY_MODEL_c83e95320c3e4e7f981a6c3395e56413",
       "IPY_MODEL_1e18fb576c5341a797f2976b93083567"
      ],
      "layout": "IPY_MODEL_347798a6e8d4481994403d3181ad262c"
     }
    },
    "2897ddc51fe34f6da197a987c370af7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_46c26e92b98c420aba7f14567adf178a",
      "placeholder": "​",
      "style": "IPY_MODEL_05349c36cd2f4f9d9e9f14591adf1ff2",
      "value": "model-00004-of-00004.safetensors: 100%"
     }
    },
    "28e6e807c4284b3c8e6f986574ab14a6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ac60e8b7a6e4ad1b269234053f60348": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7d5f66214dc94821ae4a30cb533de6db",
      "max": 3556377672,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c792845626bd4cc581dbfb99942d3e6b",
      "value": 3556377672
     }
    },
    "2ec43733cfba4692ae31450e1fa39ec7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "347798a6e8d4481994403d3181ad262c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3597e78f86cb49faa3b5dd8a394c214e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b6211beb59f94b0893076fcf453a1aa1",
      "placeholder": "​",
      "style": "IPY_MODEL_93ad41a5a2584e93aff46e6afeb6d717",
      "value": " 2/4 [00:45&lt;00:44, 22.39s/it]"
     }
    },
    "359a9e6aa0244cb582819eff8073409f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "39851731015e402ba99babe997542d8f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3ba46107c5c64a30bbd13526c06aab8f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_28e6e807c4284b3c8e6f986574ab14a6",
      "placeholder": "​",
      "style": "IPY_MODEL_f0404a50377549758eef6bdef5d23844",
      "value": "Loading checkpoint shards:  50%"
     }
    },
    "3bb6fb8571c2426480acfd4ab9f78946": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "40f585769c6b439790f3958ed8cb5c8e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "46c26e92b98c420aba7f14567adf178a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4789716411624322897cd3e953645032": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_08d06d1cbe0e4dae9ec7704b87352b50",
      "max": 3864726424,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_564414325fa14761be8c51b3750df8c2",
      "value": 3864726424
     }
    },
    "4aaaa012dc9c4153babd085fdd327a2d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "514548eb711844c7999764cb5c0292d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "52ce361d1d0f401aa6f55d01726afad2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d617764916c944549e1d12de7ca697b9",
      "placeholder": "​",
      "style": "IPY_MODEL_9fc88edea9b54d98b0f7c63e39d5ccb7",
      "value": "model-00002-of-00004.safetensors: 100%"
     }
    },
    "559f7830897a4717a80792c981a23140": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_359a9e6aa0244cb582819eff8073409f",
      "placeholder": "​",
      "style": "IPY_MODEL_67659047e0664189963c00777cb23e12",
      "value": " 663/663 [00:00&lt;00:00, 61.7kB/s]"
     }
    },
    "564414325fa14761be8c51b3750df8c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "61c673337b92430bb5a5db98dabb11b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "67659047e0664189963c00777cb23e12": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6fff62f34e784c76b77219ff8247eb9d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "72f4788446e541df8c206ad35ae9d829": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "752a926ef8b64cf58ba7ef768c13e0e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "78ee1cb931ee459eb0692162b7e6ecde": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a735222f183d4603848dd889aa574bd6",
       "IPY_MODEL_4789716411624322897cd3e953645032",
       "IPY_MODEL_b6203b8865b34f11a9e428c18c0299a4"
      ],
      "layout": "IPY_MODEL_0610812e4f764ad3b7c70e691042ff9e"
     }
    },
    "7affa41f6cc04c67be1bf2f7634824f6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d5f66214dc94821ae4a30cb533de6db": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "863ca2cfd7d94d29ad206135f01b4b00": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8a9a0690e9f74e8588109de567767628": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b4251136bc64162a771984eed1b55a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "93ad41a5a2584e93aff46e6afeb6d717": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9589dd0a2ac64279b85d0be744526814": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "96bd8a5c73ad4d31bb2eddea01293de2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b5acce17ee9d4c458d30e47300b34bfc",
       "IPY_MODEL_a94e41876c5d47769f72466e6a4c3421",
       "IPY_MODEL_dff20f3a699549d5b4402db30f6d0720"
      ],
      "layout": "IPY_MODEL_ffc38f690b0b4357a93a1a0efdeb51a4"
     }
    },
    "9b801df0838f47c4b7ca7d7400aa5608": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9bd50b6ca8884244bac71e00f597742a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9cc00eda7d654394879e47b7773ab2bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9f5c1db60551497897fe4c4d1d7521d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9fc88edea9b54d98b0f7c63e39d5ccb7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a0103dce9f63494c9151cece74ecd1b0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a21617182b2b42f3bfe53c5a95a68f7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a735222f183d4603848dd889aa574bd6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8a9a0690e9f74e8588109de567767628",
      "placeholder": "​",
      "style": "IPY_MODEL_514548eb711844c7999764cb5c0292d2",
      "value": "model-00003-of-00004.safetensors: 100%"
     }
    },
    "a8cbd066ea6449af81f27d33514e7c0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d2cef355731e40548ae1a4ea3e1accb4",
      "placeholder": "​",
      "style": "IPY_MODEL_9bd50b6ca8884244bac71e00f597742a",
      "value": " 3.56G/3.56G [01:01&lt;00:00, 133MB/s]"
     }
    },
    "a94e41876c5d47769f72466e6a4c3421": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_043fc5ff138f47a185c9f1eff3449fbd",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_22531ce7b67d4567acf1c967de042e27",
      "value": 4
     }
    },
    "aa9da7977aa54c2fbc186f05b99b2b4b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ac5a531e731e4728856d4aee4c72ead3",
      "placeholder": "​",
      "style": "IPY_MODEL_863ca2cfd7d94d29ad206135f01b4b00",
      "value": "config.json: 100%"
     }
    },
    "ab063249517846c2b14830e0c73bf2a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bae4d15ecd58450dac6dbc225a0883d2",
      "placeholder": "​",
      "style": "IPY_MODEL_f72d0feb81454dcb8fa2d45d7862c3a4",
      "value": "model-00001-of-00004.safetensors: 100%"
     }
    },
    "ac5a531e731e4728856d4aee4c72ead3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b5acce17ee9d4c458d30e47300b34bfc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_39851731015e402ba99babe997542d8f",
      "placeholder": "​",
      "style": "IPY_MODEL_61c673337b92430bb5a5db98dabb11b3",
      "value": "Downloading shards: 100%"
     }
    },
    "b6203b8865b34f11a9e428c18c0299a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dfec62ff39844c6d96c9b3278b15eda6",
      "placeholder": "​",
      "style": "IPY_MODEL_9589dd0a2ac64279b85d0be744526814",
      "value": " 3.86G/3.86G [01:04&lt;00:00, 84.9MB/s]"
     }
    },
    "b6211beb59f94b0893076fcf453a1aa1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b6490f0a12af4271bcb6bf17e6aab59c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bae4d15ecd58450dac6dbc225a0883d2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bfaac2bc1cb84cd4b0a281eece1dcbd0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c11412d165fd4380b9392eabf88af442": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3ba46107c5c64a30bbd13526c06aab8f",
       "IPY_MODEL_f7039a7aa4cd4151ac3ba59ff60a4ae6",
       "IPY_MODEL_3597e78f86cb49faa3b5dd8a394c214e"
      ],
      "layout": "IPY_MODEL_9b801df0838f47c4b7ca7d7400aa5608"
     }
    },
    "c792845626bd4cc581dbfb99942d3e6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c83e95320c3e4e7f981a6c3395e56413": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4aaaa012dc9c4153babd085fdd327a2d",
      "max": 27752,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_da95310ded27490cbc5fadec1a0fb021",
      "value": 27752
     }
    },
    "c959a9823488489bb2f22b47c951c9e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ab063249517846c2b14830e0c73bf2a3",
       "IPY_MODEL_137f65ad3b414cd2856f658be634d375",
       "IPY_MODEL_280d17af15684a5797a9f34f1c6c00d0"
      ],
      "layout": "IPY_MODEL_72f4788446e541df8c206ad35ae9d829"
     }
    },
    "cd49d122f5404e1e9574774d1bb3af7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7affa41f6cc04c67be1bf2f7634824f6",
      "max": 3864726352,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b6490f0a12af4271bcb6bf17e6aab59c",
      "value": 3864726352
     }
    },
    "d2cef355731e40548ae1a4ea3e1accb4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d617764916c944549e1d12de7ca697b9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da95310ded27490cbc5fadec1a0fb021": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "db360d49a6b245369229d963a729c87d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dfec62ff39844c6d96c9b3278b15eda6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dff20f3a699549d5b4402db30f6d0720": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6fff62f34e784c76b77219ff8247eb9d",
      "placeholder": "​",
      "style": "IPY_MODEL_a21617182b2b42f3bfe53c5a95a68f7f",
      "value": " 4/4 [04:27&lt;00:00, 65.54s/it]"
     }
    },
    "e9763ce1b908421c949beb8efab138b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ed60e5c0420c43a29935b1f0f98d73cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0103dce9f63494c9151cece74ecd1b0",
      "placeholder": "​",
      "style": "IPY_MODEL_9cc00eda7d654394879e47b7773ab2bc",
      "value": "model.safetensors.index.json: 100%"
     }
    },
    "f0404a50377549758eef6bdef5d23844": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f0db2efd24904f09aec4b2889f6b935d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f22d5ad6a68449a4af0cd4e502457a3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2897ddc51fe34f6da197a987c370af7e",
       "IPY_MODEL_2ac60e8b7a6e4ad1b269234053f60348",
       "IPY_MODEL_a8cbd066ea6449af81f27d33514e7c0c"
      ],
      "layout": "IPY_MODEL_8b4251136bc64162a771984eed1b55a8"
     }
    },
    "f2f35747ea7a4fb1a2196c54cfb3f8cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f5b4cffdaf4f42788c90294ec95d3984": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f0db2efd24904f09aec4b2889f6b935d",
      "placeholder": "​",
      "style": "IPY_MODEL_f2f35747ea7a4fb1a2196c54cfb3f8cd",
      "value": " 3.86G/3.86G [01:12&lt;00:00, 101MB/s]"
     }
    },
    "f7039a7aa4cd4151ac3ba59ff60a4ae6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2ec43733cfba4692ae31450e1fa39ec7",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e9763ce1b908421c949beb8efab138b2",
      "value": 2
     }
    },
    "f72d0feb81454dcb8fa2d45d7862c3a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fc77af29a9bc4f199a60501250d1c08f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ffc38f690b0b4357a93a1a0efdeb51a4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
